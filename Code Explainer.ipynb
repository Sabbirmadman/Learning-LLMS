{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## First Import the necessary pkages also add your own api keys to the Secrets\n",
        "\n"
      ],
      "metadata": {
        "id": "9D8PFSBs23pG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYADI2we4Avq",
        "outputId": "fbeff98f-83df-40fe-a33a-b59880a7a6f1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.18.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.27.2)\n",
            "Downloading groq-0.18.0-py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EjHve6bJ2oSV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "from typing import List\n",
        "from bs4 import BeautifulSoup\n",
        "from IPython.display import Markdown, display, update_display\n",
        "from openai import OpenAI #if you wanna use open ai\n",
        "from groq import Groq #if you wanna use GroqCloud for free models like llama\n",
        "from IPython.display import Markdown, display #This is for viasualising the markdown format"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aybXyxqpHvOn",
        "outputId": "66cf81a8-23a5-459a-a6e4-855bf6d56c12"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.15.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.8-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.7.0 (from gradio)\n",
            "  Downloading gradio_client-1.7.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Collecting huggingface-hub>=0.28.1 (from gradio)\n",
            "  Downloading huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.5)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.9.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.0->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.0->gradio) (14.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.17.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.15.0-py3-none-any.whl (57.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.7.0-py3-none-any.whl (321 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.9/321.9 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.8-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.28.1-py3-none-any.whl (464 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m464.1/464.1 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.9.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, aiofiles, starlette, huggingface-hub, safehttpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.27.1\n",
            "    Uninstalling huggingface-hub-0.27.1:\n",
            "      Successfully uninstalled huggingface-hub-0.27.1\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.8 ffmpy-0.5.0 gradio-5.15.0 gradio-client-1.7.0 huggingface-hub-0.28.1 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.20 ruff-0.9.4 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.45.3 tomlkit-0.13.2 uvicorn-0.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "gTpmfxC6IFtS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the groq or open ai api key from Secrets"
      ],
      "metadata": {
        "id": "D35oiTQH3k5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('GROQ_API_KEY')\n",
        "# userdata.get('OPEN_AI_API_KEY')\n",
        "\n",
        "# Check the key\n",
        "if not api_key:\n",
        "    print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
        "else:\n",
        "    print(\"API key found and looks good so far!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_Zv_xtf222q",
        "outputId": "47474fcb-e5cc-4ab1-893b-51d8a5f7cfe7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API key found and looks good so far!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# constants for groq i am using lama 70b model\n",
        "MODEL_GPT = 'gpt-4o-mini'\n",
        "MODEL_LLAMA = 'llama-3.3-70b-versatile'"
      ],
      "metadata": {
        "id": "gRxkoNAm31Vp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Chat Clients for the models"
      ],
      "metadata": {
        "id": "0_WRk01t41FL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = Groq(api_key=api_key)\n",
        "# client = OpenAI(api_key=api_key)\n",
        "\n",
        "#check if the client sends a response\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL_LLAMA,\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3ySRUNV40oC",
        "outputId": "8b03631c-9abb-495c-f601-21734fd63197"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Los Angeles Dodgers won the 2020 World Series. They defeated the Tampa Bay Rays in the series, winning 4 games to 2. This was the Dodgers' first World Series title since 1988. The final game was played on October 27, 2020, at Globe Life Field in Arlington, Texas.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now using prompting style we will define User prompt and system prompt.\n",
        "A **system prompt** tells the model what is it and how should it reply where **user prompt** means based on what query the model should reply.\n"
      ],
      "metadata": {
        "id": "yjT_qdKl6kIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"You are an assistant that analyzes code provided by a user and explain the code\n",
        "line by line so that its very easy to understand for the user. Respond in markdown.\"\"\"\n",
        "user_prompt = \"\"\"\n",
        "Please explain what this code does and why:\n",
        "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "kr8P2qhD7G5M"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lets see it in action\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL_LLAMA,\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt},\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Z9c3h0VV5Odg",
        "outputId": "2e3da932-031f-411c-cc29-010a35e91285"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Explanation of the Code\n",
            "The given code is a Python expression that uses a combination of dictionary methods, set comprehension, and the `yield from` statement. Let's break it down:\n",
            "\n",
            "```python\n",
            "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
            "```\n",
            "\n",
            "* `for book in books`: This part of the expression iterates over each `book` in the `books` collection. The `books` collection is assumed to be a list or other iterable of dictionaries, where each dictionary represents a book.\n",
            "* `if book.get(\"author\")`: This is a conditional statement that checks if the `book` dictionary has a key named `\"author\"` and if its value is truthy. The `get()` method is used to avoid a KeyError if the key does not exist. If the condition is true, the expression inside the set comprehension is evaluated.\n",
            "* `book.get(\"author\")`: This expression retrieves the value associated with the `\"author\"` key in the `book` dictionary.\n",
            "* `{...}`: This is a set comprehension, which creates a set containing the results of the expression inside the braces. Sets in Python are unordered collections of unique elements.\n",
            "* `yield from`: This statement is used in generators to yield values from a sub-iterator. It allows the generator to produce a series of values, rather than a single value.\n",
            "\n",
            "### Purpose of the Code\n",
            "The purpose of this code is to generate a sequence of unique author names from a collection of books. The `yield from` statement is used to produce this sequence, which can be iterated over in a larger context.\n",
            "\n",
            "### Why This Code is Useful\n",
            "This code is useful when:\n",
            "\n",
            "* You have a collection of books and you want to get a list of unique author names.\n",
            "* You want to avoid duplicates in the list of authors.\n",
            "* You want to iterate over the authors in a larger context, such as in a loop or in a pipeline of data processing operations.\n",
            "\n",
            "### Example Use Case\n",
            "Here is an example of how this code might be used in a larger context:\n",
            "```python\n",
            "def get_authors(books):\n",
            "    yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
            "\n",
            "books = [\n",
            "    {\"title\": \"Book 1\", \"author\": \"Author A\"},\n",
            "    {\"title\": \"Book 2\", \"author\": \"Author B\"},\n",
            "    {\"title\": \"Book 3\", \"author\": \"Author A\"},\n",
            "    {\"title\": \"Book 4\"},\n",
            "]\n",
            "\n",
            "authors = list(get_authors(books))\n",
            "print(authors)  # Output: [\"Author A\", \"Author B\"]\n",
            "```\n",
            "In this example, the `get_authors` function uses the code in question to generate a sequence of unique author names from the `books` collection. The `list` function is used to materialize the sequence into a list, which is then printed to the console.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(response.choices[0].message.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 779
        },
        "collapsed": true,
        "id": "hqRALT-98yyY",
        "outputId": "f750b020-f10f-4025-f047-0bf282eb4118"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Explanation of the Code\nThe given code is a Python expression that uses a combination of dictionary methods, set comprehension, and the `yield from` statement. Let's break it down:\n\n```python\nyield from {book.get(\"author\") for book in books if book.get(\"author\")}\n```\n\n* `for book in books`: This part of the expression iterates over each `book` in the `books` collection. The `books` collection is assumed to be a list or other iterable of dictionaries, where each dictionary represents a book.\n* `if book.get(\"author\")`: This is a conditional statement that checks if the `book` dictionary has a key named `\"author\"` and if its value is truthy. The `get()` method is used to avoid a KeyError if the key does not exist. If the condition is true, the expression inside the set comprehension is evaluated.\n* `book.get(\"author\")`: This expression retrieves the value associated with the `\"author\"` key in the `book` dictionary.\n* `{...}`: This is a set comprehension, which creates a set containing the results of the expression inside the braces. Sets in Python are unordered collections of unique elements.\n* `yield from`: This statement is used in generators to yield values from a sub-iterator. It allows the generator to produce a series of values, rather than a single value.\n\n### Purpose of the Code\nThe purpose of this code is to generate a sequence of unique author names from a collection of books. The `yield from` statement is used to produce this sequence, which can be iterated over in a larger context.\n\n### Why This Code is Useful\nThis code is useful when:\n\n* You have a collection of books and you want to get a list of unique author names.\n* You want to avoid duplicates in the list of authors.\n* You want to iterate over the authors in a larger context, such as in a loop or in a pipeline of data processing operations.\n\n### Example Use Case\nHere is an example of how this code might be used in a larger context:\n```python\ndef get_authors(books):\n    yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n\nbooks = [\n    {\"title\": \"Book 1\", \"author\": \"Author A\"},\n    {\"title\": \"Book 2\", \"author\": \"Author B\"},\n    {\"title\": \"Book 3\", \"author\": \"Author A\"},\n    {\"title\": \"Book 4\"},\n]\n\nauthors = list(get_authors(books))\nprint(authors)  # Output: [\"Author A\", \"Author B\"]\n```\nIn this example, the `get_authors` function uses the code in question to generate a sequence of unique author names from the `books` collection. The `list` function is used to materialize the sequence into a list, which is then printed to the console."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lets modularise the code"
      ],
      "metadata": {
        "id": "OS3MkaU--E_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"You are an assistant that analyzes code provided by a user and explain the code\n",
        "line by line so that its very easy to understand for the user. Respond in markdown.\"\"\""
      ],
      "metadata": {
        "id": "VGXhWbR0-oGR"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def user_prompt_generator(code_snippets: str) -> str:\n",
        "    return f\"\"\"\n",
        "    Please explain what this code does and why:\n",
        "    {code_snippets}\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "dKZ1xhkf9CUo"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def messages_for(code_snippets: str) -> str:\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt_generator(code_snippets)}\n",
        "    ]"
      ],
      "metadata": {
        "id": "c8ijTxY7-nF3"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def code_analyzer(MODEL: str, code_snippets: str ) -> str:\n",
        "    response = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages = messages_for(code_snippets)\n",
        "    )\n",
        "    bot_response =  response.choices[0].message.content\n",
        "    display(Markdown(bot_response))\n",
        "    return bot_response"
      ],
      "metadata": {
        "id": "nzR356Jd-907"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this will reply as chat gpt stream output\n",
        "def code_analyzer_stream_output(MODEL: str, code_snippets: str ) -> str:\n",
        "    stream = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages = messages_for(code_snippets),\n",
        "        stream=True\n",
        "    )\n",
        "    response = \"\"\n",
        "    display_handle = display(Markdown(\"\"), display_id=True)\n",
        "    for chunk in stream:\n",
        "        response += chunk.choices[0].delta.content or ''\n",
        "        response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
        "        update_display(Markdown(response), display_id=display_handle.display_id)"
      ],
      "metadata": {
        "id": "GDXNB1uzAPQi"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "userInput = input(\"Enter the code snippet you want to analyze: \")\n",
        "code_analyzer(MODEL_LLAMA, userInput)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "AR6POYyk_O--",
        "outputId": "4268f9cd-352d-4921-e566-2d3f9067bb04"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the code snippet you want to analyze: headers = {  \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\" }  class Website:      def __init__(self, url):         \"\"\"         Create this Website object from the given url using the BeautifulSoup library         \"\"\"         self.url = url         response = requests.get(url, headers=headers)         soup = BeautifulSoup(response.content, 'html.parser')         self.title = soup.title.string if soup.title else \"No title found\"         for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):             irrelevant.decompose()         self.text = soup.body.get_text(separator=\"\\n\", strip=True)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Code Explanation\n#### Importing Libraries and Setting User Agent\n```python\nheaders = {  \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\" }\n```\nThis line sets a dictionary `headers` with a `\"User-Agent\"` key. The value of this key is a string that identifies the browser and operating system of the client making the HTTP request. This is done to mimic the behavior of a real web browser and avoid being blocked by websites that don't want to be scraped.\n\n#### Defining the Website Class\n```python\nclass Website:\n```\nThis line defines a new class called `Website`.\n\n#### Initializing the Website Object\n```python\ndef __init__(self, url):\n```\nThis is the constructor method of the `Website` class, which is called when an object of this class is instantiated. The `url` parameter is the URL of the website that the object will represent.\n\n#### Docstring for the Constructor Method\n```python\n\"\"\"Create this Website object from the given url using the BeautifulSoup library\"\"\"\n```\nThis is a docstring that provides a description of the constructor method. It explains that the method creates a `Website` object from a given URL using the BeautifulSoup library.\n\n#### Sending an HTTP Request and Getting the Response\n```python\nself.url = url\nresponse = requests.get(url, headers=headers)\n```\nHere, the `url` attribute of the `Website` object is set to the provided `url`. Then, an HTTP GET request is sent to the specified `url` with the previously defined `headers`. The response from the server is stored in the `response` variable.\n\n#### Parsing the HTML Content\n```python\nsoup = BeautifulSoup(response.content, 'html.parser')\n```\nThe HTML content of the response is parsed using the `BeautifulSoup` library, which creates a parse tree that can be used to extract data from the HTML.\n\n#### Extracting the Title of the Website\n```python\nself.title = soup.title.string if soup.title else \"No title found\"\n```\nThis line extracts the title of the website from the parse tree. If the website has a title, it is stored in the `title` attribute of the `Website` object. If no title is found, the string \"No title found\" is stored instead.\n\n#### Removing Irrelevant Elements from the Parse Tree\n```python\nfor irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n    irrelevant.decompose()\n```\nThis loop goes through all the elements in the body of the HTML that are of type `script`, `style`, `img`, or `input`. These elements are considered irrelevant because they do not contain text that is visible to the user. The `decompose` method is used to remove these elements from the parse tree.\n\n#### Extracting the Text from the Parse Tree\n```python\nself.text = soup.body.get_text(separator=\"\\n\", strip=True)\n```\nFinally, this line extracts the text from the parse tree, excluding any irrelevant elements that were removed in the previous step. The `separator` parameter is set to `\\n` to separate the text with newline characters, and the `strip` parameter is set to `True` to remove any leading or trailing whitespace from the text. The extracted text is stored in the `text` attribute of the `Website` object.\n\n### Why This Code is Useful\nThis code is useful for scraping websites and extracting relevant information from them. By removing irrelevant elements like scripts, styles, images, and input fields, it makes it easier to extract the actual text content of a website. This can be useful for tasks like text analysis, sentiment analysis, or data mining. Additionally, by setting a User-Agent header, the code can avoid being blocked by websites that don't want to be scraped."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "userInput = input(\"Enter the code snippet you want to analyze: \")\n",
        "code_analyzer_stream_output(MODEL_LLAMA, userInput)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 980
        },
        "id": "zxML9LpaApsE",
        "outputId": "61d9448f-8850-4fa1-8134-e4cd939c9c83"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the code snippet you want to analyze: def stream_brochure(company_name, url):     stream = Groqclient.chat.completions.create(         model=MODEL,         messages=[             {\"role\": \"system\", \"content\": system_prompt},             {\"role\": \"user\", \"content\": get_brochure_user_prompt(company_name, url)}           ],         stream=True     )          response = \"\"     display_handle = display(Markdown(\"\"), display_id=True)     for chunk in stream:         response += chunk.choices[0].delta.content or ''         response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")         update_display(Markdown(response), display_id=display_handle.display_id)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Code Explanation\n#### Function Definition\npython\ndef stream_brochure(company_name, url):\n\nThis line defines a function named `stream_brochure` that takes two parameters: `company_name` and `url`.\n\n#### Initialization of Stream\npython\nstream = Groqclient.chat.completions.create(\n\nHere, the function `stream_brochure` is using the `Groqclient` library to create a stream of completions. The `Groqclient` library is likely an API client for a language model or chat service.\n\n#### Stream Configuration\npython\nmodel=MODEL,\nmessages=[\n    {\"role\": \"system\", \"content\": system_prompt},\n    {\"role\": \"user\", \"content\": get_brochure_user_prompt(company_name, url)}\n],\nstream=True\n\nIn this section, the function is configuring the stream with the following settings:\n* `model`: The language model to use for generating completions. The `MODEL` variable is not defined in this snippet, but it's likely a pre-trained model.\n* `messages`: A list of messages to use as input for the language model. The list contains two messages:\n\t+ The first message is a system prompt, which is a predefined string stored in the `system_prompt` variable.\n\t+ The second message is a user prompt generated by the `get_brochure_user_prompt` function, which takes `company_name` and `url` as input.\n* `stream=True`: This setting enables streaming mode, which allows the function to receive a continuous stream of completions from the language model.\n\n#### Response Initialization\npython\nresponse = \"\"\ndisplay_handle = display(Markdown(\"\"), display_id=True)\n\nHere, the function initializes an empty string `response` to store the generated completion. It also creates a display handle using the `display` function from a library like Jupyter Notebook or a similar environment. The display handle is used to update the output in real-time.\n\n#### Streaming Completions\npython\nfor chunk in stream:\n    response += chunk.choices[0].delta.content or ''\n\nThis loop iterates over the stream of completions generated by the language model. For each chunk, it extracts the content of the first choice (i.e., the most likely completion) and appends it to the `response` string.\n\n#### Response Processing\npython\nresponse = response.replace(\"\",\"\").replace(\"\", \"\")\n\nHere, the function processes the `response` string by removing backticks (`` ` ``) and the string \"\".\n\n#### Updating Display\npython\nupdate_display(Markdown(response), display_id=display_handle.display_id)\n\nFinally, the function updates the display using the `update_display` function, passing the processed `response` string as Markdown content and the display ID from the display handle. This updates the output in real-time, allowing the user to see the generated completion as it is being generated.\n\n### Why This Code Exists\nThis code is likely part of a larger application that uses a language model to generate content, such as a brochure, based on a company name and URL. The `stream_brochure` function is designed to generate this content in real-time, using a streaming API to receive a continuous stream of completions from the language model. The function processes the generated content, removes unwanted characters, and updates the display in real-time, allowing the user to see the generated brochure as it is being generated."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lets create UI**"
      ],
      "metadata": {
        "id": "ppF5YZMuIKCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# as a reference lets create a simple interface using gradio\n",
        "def shout(text):\n",
        "    print(f\"Shout has been called with input {text}\")\n",
        "    return text.upper()\n",
        "\n",
        "gr.Interface(\n",
        "    fn=shout,\n",
        "    inputs=[gr.Textbox(label=\"Your message:\")],\n",
        "    outputs=[gr.Textbox(label=\"Response:\")],\n",
        "    flagging_mode=\"never\"\n",
        "  ).launch(\n",
        "    share=True,\n",
        "    debug=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "PBzVtjKzINNC",
        "outputId": "64c87685-3e99-4717-e9a4-dfcc760c202a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://af33543f2dcc88cef5.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://af33543f2dcc88cef5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7862 <> https://af33543f2dcc88cef5.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def code_analyzer_for_interface(code_snippets: str ) -> str:\n",
        "    response = client.chat.completions.create(\n",
        "        model=MODEL_LLAMA,\n",
        "        messages = messages_for(code_snippets)\n",
        "    )\n",
        "    bot_response =  response.choices[0].message.content\n",
        "    return bot_response"
      ],
      "metadata": {
        "id": "KOvQ8df2K23l"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gr.Interface(\n",
        "    fn=code_analyzer_for_interface,\n",
        "    inputs=[gr.Textbox(label=\"Your message:\", lines=6)],\n",
        "    outputs=[gr.Textbox(label=\"Response:\", lines=8)],\n",
        "    flagging_mode=\"never\"\n",
        "  ).launch(\n",
        "    share=True,\n",
        "    debug=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "a2cuNNJBIP7J",
        "outputId": "d7442590-39f0-4491-f2c1-f9fc9e0ae158"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://44fa5effcba18752af.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://44fa5effcba18752af.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7862 <> https://44fa5effcba18752af.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lets show the response in markdown\n",
        "gr.Interface(\n",
        "    fn=code_analyzer_for_interface,\n",
        "    inputs=[gr.Textbox(label=\"Your code :\", lines=6)],\n",
        "    outputs=[gr.Markdown(label=\"Response:\")],\n",
        "    flagging_mode=\"never\"\n",
        "  ).launch(\n",
        "    share=True,\n",
        "    debug=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "oz0F4V1bLn3Q",
        "outputId": "3ae46b34-8251-4a05-f670-7d5b0ae95837"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://f0b2426d0b1ffeebee.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f0b2426d0b1ffeebee.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7862 <> https://f0b2426d0b1ffeebee.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#i want also to stream the response\n",
        "def code_analyzer_stream_output_for_interface(code_snippets: str ) -> str:\n",
        "    stream = client.chat.completions.create(\n",
        "        model=MODEL_LLAMA,\n",
        "        messages = messages_for(code_snippets),\n",
        "        stream=True\n",
        "    )\n",
        "    result = \"\"\n",
        "    for chunk in stream:\n",
        "        result += chunk.choices[0].delta.content or \"\"\n",
        "        yield result"
      ],
      "metadata": {
        "id": "tBmsxEPQLwC-"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "view = gr.Interface(\n",
        "    fn=code_analyzer_stream_output_for_interface,\n",
        "    inputs=[gr.Textbox(label=\"Your message:\")],\n",
        "    outputs=[gr.Markdown(label=\"Response:\")],\n",
        "    flagging_mode=\"never\"\n",
        ")\n",
        "view.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "bJSyY93OMKC4",
        "outputId": "c483ab3d-d8f9-4ac3-b71e-16460aa52937"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://482b58c0b3cd81c034.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://482b58c0b3cd81c034.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## As we are using groq it has many open source models. lets add dropdown to select models and do minor improvements"
      ],
      "metadata": {
        "id": "2BexbuB9NP4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#i want also to stream the response\n",
        "def code_analyzer_stream_output_for_interface(code_snippets: str , model : str ) -> str:\n",
        "    print(model)\n",
        "    print(messages_for(code_snippets))\n",
        "\n",
        "    stream = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages = messages_for(code_snippets),\n",
        "        stream=True\n",
        "    )\n",
        "    result = \"\"\n",
        "    for chunk in stream:\n",
        "        result += chunk.choices[0].delta.content or \"\"\n",
        "        yield result"
      ],
      "metadata": {
        "id": "Dc6VG6A3O-QL"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "view = gr.Interface(\n",
        "    fn=code_analyzer_stream_output_for_interface,\n",
        "    inputs=[gr.Textbox(label=\"Your Code:\"), gr.Dropdown([\"llama-3.3-70b-versatile\",\"deepseek-r1-distill-llama-70b\",\"gemma2-9b-it\"], label=\"Select model\", value=\"llama-3.3-70b-versatile\")],\n",
        "    outputs=[gr.Markdown(label=\"Response:\")],\n",
        "    flagging_mode=\"never\"\n",
        ")\n",
        "view.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "_eCogYYON7LY",
        "outputId": "2d271b7f-fa31-4011-d474-153ff868afd1"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://722375079402a0f659.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://722375079402a0f659.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    }
  ]
}